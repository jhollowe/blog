[{"content":"I first noticed Kernel Same-page Merging (KSM) while working with Virtual Machines (VMs) under KVM (in Proxmox VE).\nKSM is a way of reducing physical memory usage by using one physical page of memory for all duplicate copied of that page. It does this by periodically scanning through memory, finding duplicate pages, and de-duplicating them via virtual memory. It is an extension of how the kernel shares pages between fork()\u0026lsquo;ed processes and uses many of the same methods of sharing memory. KSM is most often used with virtualization to de-duplicate memory used by guest Operating Systems (OSs), but can be used for any page of memory which the program registers with KSM to scan. \u0026ldquo;Red Hat found that thanks to KSM, KVM can run as many as 52 Windows XP VMs with 1 GB of RAM each on a server with just 16 GB of RAM.\u0026quot;1\nVirtual Memory Background To fully understand how KSM works, a (at least) basic understanding of how virtual memory work is required.\nTo prevent programs from having to know where every other process on the computer is using memory, the kernel (the all-powerful dictator of the OS) tells each process it has memory starting at address 0. It then keeps a record of where in actual (physical) memory each block (page) or the virtual memory is located.\nIt uses this mapping to translate memory addresses each time the process reads or writes to memory.\nÂ© Computer History Museum    This virtual memory also allows things like memory-mapped files on disk and Copy-On-Write (COW) pages. When a process clones (forks) itself, it doesn\u0026rsquo;t have to make a copy of all the memory it was using. It simply marks each page as COW. Each process can read from their memory with both virtual addresses pointing to the same physical page (now marked COW), but when either attempts to write to memory, the existing physical page is left inn place (so the other process can still use it) and a new physical page is allocated and mapped to the writer\u0026rsquo;s virtual memory. This allows pages of memory that are not changed in forked processes to use no additional memory.\nthe same process is used by KSM: it finds duplicate pages in the memory ranges registered with it, marks one of the physical pages as COW, and frees the other physical pages after mapping all the virtual pages to the one physical page.\n https://kernelnewbies.org/Linux_2_6_32#Kernel_Samepage_Merging_.28memory_deduplication.29\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","description":"Today I Learned about Kernel Same-page Merging (KSM)","id":0,"section":"posts","tags":["Linux","memory"],"title":"TIL: Kernel Same-page Merging (KSM)","uri":"https://johnhollowell.com/blog/posts/til-ksm/"},{"content":"Please read A Trickle Is Better Than Nothing before reading this post. I just got over having no internet at my apartment for over a week. I was gone a portion of the week, but it was still very inconvenient. Working remotely doesn\u0026rsquo;t help as to get paid I need to have an internet connection (but not necessarily a fast connection).\nWorking Around It While I could have use cellular data to carry me through, I had already used a significant portion of my data cap on various travels this summer. I ended up just going onto campus and working from my laptop in a computer lab.\nWhile on campus (with its wonderful gigabit symmetrical internet), I downloaded some videos from my YouTube Watch Later playlist so I could have some videos to watch at home. I tried to do as much pre-downloading of content I could so I would have it accessible at home.\nMissing the Trickle So I had everything downloaded and I was fine, right? Wrong.\nI do more with my life than just watching YouTube. I play games, I browse social media, and (most frustratingly in this situation) I code. It is impossible to stay up-to-date on PRs and Issues without being able to connect to the internet. While I could have looked at the GutHub website on my phone, I have a lot of nice tooling around Issues/PRs that is on my desktop.\nI also wanted to open some PRs on some FOSS projects I want to improve. I couldn\u0026rsquo;t do a git clone, I couldn\u0026rsquo;t download the devcontainers needed for the new project and language, I couldn\u0026rsquo;t easily research how to do what I wanted in the documentation on StackOverflow. This stopped me dead in my tracks and forced me to either make a trip back to campus to get internet or use the limited cellular data I had left to clone the entire repo and pull all the require container layers.\nWhat If How could it have been if I had at least a small amount of internet? I would still utilize the high-speed connection at campus to download some content to watch, but I would have still been able to pull up the YT page for the video to see comments and the description and to comment and like myself. While it would have taken a while, I could have left the repo and containers to download while I was watching something or making dinner or overnight. I could have refreshed my Issues/PRs and get any updates on their status and checks. I could have seen that a new video was released by my favorite channel and either queue the video to download or go somewhere with internet to quickly download it.\nOverall, I am very grateful for the internet I have. This just makes me appreciate the internet all the more with its redundancy and high availability and goes to prove that the last mile is really the most vulnerable segment of any network or connection.\n","description":"I just got over having no internet at my apartment for over a week, and I can confirm that a trickle is better than nothing.","id":1,"section":"posts","tags":["web","life","opinion"],"title":"Nothing Is Definitely Worse Than a Trickle","uri":"https://johnhollowell.com/blog/posts/nothing-is-definitely-worse-than-a-trickle/"},{"content":"Setting up and maintaining a development environment is hard, especially when you need to destructively test features or libraries. Especially for contributing to a new project, you don\u0026rsquo;t know everything that is needed. Sometimes the install/development instructions assume some base tools or packages that are not included in your development environment of choice.\nIn come devcontainers. Rather than having to search through the README for a project you are wanting to contribute to, installing several packages onto your machine, and troubleshooting when it doesn\u0026rsquo;t work, you can simply open the repository as a devcontainer and you are ready to start contributing. Have a project that requires several separate services (databases, middleware/api server, etc.)? Create a devcontainer using docker-compose and your development environment can launch an entire suit of containers exactly how you need them.\nSetup Install Docker To be able to use containers, we need a container manager: Docker.\nTo get Docker installed, simply follow their instructions\nInstall VS Code To get Visual Studio Code (VS Code) installed, simply follow their instructions\nAdd container remote extension Within VS Code, install the Remote - Containers extension\n Click the Extensions sidebar (or use the \u0026ldquo;Ctrl + Shift + X\u0026rdquo; shortcut) Search for ms-vscode-remote.remote-containers Click \u0026ldquo;Install\u0026rdquo;  Test It Out Now that you are ready to use a devcontainer, it is time to test it out!\nYou can grab this blog and use it as the devcontainer to play with. Click on the bottom left in VS Code on the green arrows, find the Container remote section, and select \u0026ldquo;Clone Repository in Container Volume\u0026hellip;\u0026rdquo;, enter https://github.com/jhollowe/blog and hit enter.\nAfter a minute or so of downloading and building your development container, VS Code will be fully functional. You can use the included tasks (Terminal \u0026gt; Run Task\u0026hellip; \u0026gt; Serve) to build and serve the blog. The devcontainer includes everything needed to build the blog and run VS Code. VS Code will even pull in common configuration for tools like Git and SSH.\nModes There are several \u0026ldquo;modes\u0026rdquo; of how to store your files in which you can use devcontainers, each with its own benefits and drawbacks.\n   \u0026ldquo;mode\u0026rdquo; Pros Cons     container volume * fast\n* fully self-contained environment * hard to access files from outside container   mounting a directory * easy to get files in and out\n* allows statefull local files * slow file I/O\n* add/edits/deletes affect the source directory   cloning a directory * as fast as a container volume\n* easy to get files into container\n* edits/deletes do not affect the source directory * hard to get files out of container    ","description":"Setting up and maintaining a development environment is hard, especially when you need to destructively test features or libraries.","id":2,"section":"posts","tags":["development","containers"],"title":"Getting Started With Devcontainers","uri":"https://johnhollowell.com/blog/posts/getting-started-with-devcontainers/"},{"content":"For environments with complex Active Directory (AD) environments, AD forests can allow flexibility in management and organization of objects.\nBasically, an AD forest allows multiple domains and trees of domains (subdomains) to access and have a shared configuration while still having separate domains with separate host servers.\nThey allow domains to trust and access each other while still maintain separations and boarders. I\u0026rsquo;ve seen this used to allow corporate and client domains to communicate or to have a development domain tree that trust and can cross-talk with the production domain tree while still being separate (this is less common as dev domains are usually just subdomains within the production tree).\nResources\n https://en.wikipedia.org/wiki/Active_Directory#Forests,_trees,_and_domains https://ipwithease.com/what-is-a-forest-in-active-directory/ https://www.varonis.com/blog/active-directory-forest/  ","description":"Today I Learned about Active Directory Forests","id":3,"section":"posts","tags":["Active Directory"],"title":"TIL: AD Forests","uri":"https://johnhollowell.com/blog/posts/til-ad-forests/"},{"content":"Changing a user\u0026rsquo;s username on Linux requires no processes be running under that user. This makes sense, but what if we only have that user accessible through a SSh connection? What if we don\u0026rsquo;t want to allow external access to the root account? What if the root account doesn\u0026rsquo;t have a password?\nBackground I was recently spinning up a bunch of Raspberry Pis running Ubuntu 20.04 and some VPSes also running Ubuntu 20.04. I wanted to change the username on these nodes, but only really had access to the ubuntu (sudo) account. While I know I could use a cloud-init file to create a user exactly how I want (more on that in a future post), I didn\u0026rsquo;t want to re-flash the nodes and was not able to add a cloud-init file before boot on the VPSes.\nThe Process Getting The Commands To Run So we can\u0026rsquo;t change the username of a user with running processes, but a SSH session and a bash shell both run under my user whenever I\u0026rsquo;m connected.\nThe main problem is executing a command from a user (and sudo-ing to root) while not having that user have a process running.\nUsing either of the commands below allows a command to be run as the root user which will continue running\n1 2 3 4 5  # interactive shell sudo tmux # non-interactive command sudo -s -- sh -c \u0026#34;nohup \u0026lt;command\u0026gt; \u0026amp;\u0026#34;   Now that we can have a command running as root independent of the initiating user, we need to kill everything of the user so we can run usermod commands without difficulty. We kill the processes and wait a couple seconds for them all to terminate. Then we can run whatever commands we need.\n1  ps -o pid= -u \u0026lt;current_username\u0026gt; | xargs kill \u0026amp;\u0026amp; sleep 2 \u0026amp;\u0026amp; \u0026lt;command\u0026gt;     What This Command Does   ps lists the processes running on the system  -o pid= selects only the process ID (pid) and does not create a header for the column (=) -u \u0026lt;username\u0026gt; selects only the processes running under \u0026lt;username\u0026gt;   | takes the output of the previous command (ps) and makes it the input of the following command (xargs) xargs takes a line separated list (can change the separator) and turns them into arguments for the following command (-r tells it to do nothing if its input is empty) kill takes a pid (or list of pids) and terminates the process. While kill can send different signals to processes, this uses the default signal (TERM). \u0026amp;\u0026amp; runs the following command if the preceding command exited successfully (exit code 0) sleep 2 wait 2 seconds for the killed processes to terminate    Now, we can get to actually changing the username!\nChanging The Username Now that we can run commands as root without our user running processes, we can proceed to change the username and other related tasks.\nThese commands assume you are running as root. If not, you may need to insert some sudo\u0026rsquo;s as necessary\n1 2 3 4 5 6 7 8 9 10 11  # change the user\u0026#39;s username usermod -l \u0026lt;new_username\u0026gt; \u0026lt;current_username\u0026gt; # move the user\u0026#39;s home directory usermod -d /home/\u0026lt;new_username\u0026gt; -m \u0026lt;new_username\u0026gt; # change user\u0026#39;s group name groupmod -n \u0026lt;new_username\u0026gt; \u0026lt;current_username\u0026gt; # replace username in all sudoers files (DANGER!) sed -i.bak \u0026#39;s/\u0026lt;current_username\u0026gt;/\u0026lt;new_username\u0026gt;/g\u0026#39; /etc/sudoers for f in /etc/sudoers.d/*; do sed -i.bak \u0026#39;s/\u0026lt;current_username\u0026gt;/\u0026lt;new_username\u0026gt;/g\u0026#39; $f done   Putting it all together When we put it all together (with some supporting script), we get change-username.sh as seen below:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65  #!/bin/bash  currentUser=$1 newUser=$2 if [ $# -lt 2 ]; then printf \u0026#34;Usage:\\n\\t$0\u0026lt;current_username\u0026gt; \u0026lt;new_username\u0026gt; [new_home_dir_path]\\n\u0026#34; exit 1 fi if [ $(id -u) -ne 0 ];then echo \u0026#34;Root permission needed for modifying users. Can not continue.\u0026#34; exit 2 fi newHome=\u0026#34;/home/$newUser\u0026#34; if [ $# == 3 ];then newHome=$3 fi echo \u0026#34;Changing $currentUserto $newUser\u0026#34; echo echo \u0026#34;Running this script has the possibility to break sudo (sudoers file(s)) and WILL kill all processes owned by $currentUser\u0026#34; echo \u0026#34;$currentUserwill be logged out and will need to reconnect as $newUser\u0026#34; read -n1 -s -r -p $\u0026#39;Continue [Y/n]?\\n\u0026#39; key if [ $key != \u0026#39;\u0026#39; -a $key != \u0026#39;y\u0026#39; -a $key != \u0026#39;Y\u0026#39; ]; then echo \u0026#34;Stopping; no files changed\u0026#34; exit 2 fi # put the main script in /tmp so the user\u0026#39;s home directory can be safely moved tmpFile=$(mktemp) cat \u0026gt; $tmpFile \u0026lt;\u0026lt; EOF #!/bin/bash shopt -s extglob # terminate (nicely) any process owned by $currentUser ps -o pid= -u $currentUser | xargs -r kill # wait for all processes to terminate sleep 2 # forcibly kill any processes that have not already terminated ps -o pid= -u $currentUser | xargs -r kill -s KILL # change the user\u0026#39;s username usermod -l \u0026#34;$newUser\u0026#34; \u0026#34;$currentUser\u0026#34; # move the user\u0026#39;s home directory usermod -d \u0026#34;$newHome\u0026#34; -m \u0026#34;$newUser\u0026#34; # change user\u0026#39;s group name groupmod -n \u0026#34;$newUser\u0026#34; \u0026#34;$currentUser\u0026#34; # replace username in all sudoers files sed -i.bak \u0026#39;s/\u0026#39;$currentUser\u0026#39;/\u0026#39;$newUser\u0026#39;/g\u0026#39; /etc/sudoers for f in /etc/sudoers.d/!(*.bak); do echo \u0026#34;editing \u0026#39;\\$f\u0026#39;\u0026#34; sed -i.bak \u0026#39;s/\u0026#39;$currentUser\u0026#39;/\u0026#39;$newUser\u0026#39;/g\u0026#39; \\$f # TODO fix $f not getting the file path for some reason done EOF echo \u0026#34;Putting script into $tmpFileand running\u0026#34; chmod 777 $tmpFile sudo -s -- bash -c \u0026#34;nohup $tmpFile\u0026gt;/dev/null \u0026amp;\u0026#34;     requirements     Command(s) Package     bash bash   ps, kill procps   usermod, groupmod passwd   sed sed   xargs findutils      ","description":"Changing a user's username on Linux requires no processes be running under that user. This makes sense, but what if we only have that user accessible through a SSh connection?","id":4,"section":"posts","tags":["sysadmin"],"title":"Change Username Without Separate Session","uri":"https://johnhollowell.com/blog/posts/change-username-without-separate-session/"},{"content":"One of the most important parts of a working cluster is the interconnection and communication between nodes. While the networking side will not be covered now, a very important aspect will be: passwordless SSH.\nInter-node SSH The first task to getting easy access between nodes is ensuring SSH access between all the nodes.\nWhile not necessary, I recommend adding all your nodes to the /etc/hosts file on each node. For example, the /etc/hosts file might look like\n127.0.0.1 localhost # The following lines are desirable for IPv6 capable hosts ::1 ip6-localhost ip6-loopback fe00::0 ip6-localnet ff00::0 ip6-mcastprefix ff02::1 ip6-allnodes ff02::2 ip6-allrouters ff02::3 ip6-allhosts to which I would add (using the actual IPs of the nodes)\n192.168.0.11 node01 192.168.0.12 node02 192.168.0.13 node03 192.168.0.14 node04   Automate adding to your hosts files  1 2 3 4 5 6 7 8 9  for node in localhost node02 node03 node04; do ssh $node \u0026#34;cat | sudo tee -a /etc/hosts \u0026gt; /dev/null\u0026#34; \u0026lt;\u0026lt; EOF 192.168.0.11 node01 192.168.0.12 node02 192.168.0.13 node03 192.168.0.14 node04 EOF done      After this is added to your hosts file on all your nodes, from any node you should be able to ssh node1 from any of them successfully after entering your password.\nNOTE: if you have not configured static IP addresses for your nodes, any changes to their IPs will require you changing the hosts file on all your nodes. Passwordless SSH To be able to SSH between nodes without the need for a password, you will need to create an SSH key. This will allow SSH to work in scripts and tools (MPI) without needing user interaction.\nFirst, we need to create a key. There are multiple standards of encryption you can use for SSH keys. The default is RSA, but it is generally considered to be less secure than modern standards. Therefore, these instructions will show how to create a ed25519 key. This will work on your cluster, but some (very) old systems may not support ED25519 keys (RSA keys will generally work everywhere even though they are less secure).\nTo create a key, use this command on one of your nodes:\n1  ssh-keygen -t ed25519 -a 100 -f ~/.ssh/id_ed25519 -C \u0026#34;Inter-node cluster ssh\u0026#34;   This article does a good job of breaking down what all the arguments are used for.\nNext, we need our nodes to trust the key we just created. We\u0026rsquo;ll start with getting the current node to trust the key.\n1  ssh-copy-id -i ~/.ssh/id_ed25519 localhost   NOTE: If you have already setup NFS with a shared home directory, you don\u0026rsquo;t need to do anything further; the key is accessible and trusted on all the nodes. Now we can just copy these files to all the other nodes so that they can use and will trust this key.\n1 2 3 4 5  for node in node02 node03 node04; do # list all the nodes that should get the key ssh-copy-id -i ~/.ssh/id_ed25519 $node # you will need to enter your password for this step scp ~/.ssh/id_ed25519 $node:.ssh/ ssh $node \u0026#34;chmod 600 ~/.ssh/id_ed25519\u0026#34; # ensure the key is locked down so SSH will accept it. done   And to make all the nodes trust each other\u0026rsquo;s fingerprints\n1 2 3  for node in node02 node03 node04; do scp ~/.ssh/known_hosts $node:.ssh/ done   We can check that we can SSH into all the nodes without having to enter a password:\n1 2  for node in node2 node3 node4; do ssh $node \u0026#34;hostname\u0026#34;   ","description":"One of the most important parts of a working cluster is the interconnection and communication between nodes. While the networking side will not be covered now, a very important aspect will be: passwordless SSH.","id":5,"section":"posts","tags":["SSH","cluster","networks"],"title":"Cluster SSH","uri":"https://johnhollowell.com/blog/posts/cluster-ssh/"},{"content":"So you want to build a Raspberry Pi cluster.\nThe first thing to do is determine the size of a cluster you want to build. You can go with any number greater than one, but I\u0026rsquo;ve found that 4-8 is a good sweet spot between too few nodes to get a real feel of cluster operation and too many nodes to manage and maintain. For this and following posts, I will be assuming a cluster of 4 nodes (node01 to node04).\nHardware To run a cluster you also need some supporting hardware, where N is the number of nodes (examples given as links):\n N Raspberry Pi 4 N Micro SD Cards (16GB or more preferred) 1 gigabit ethernet switch (at least N+1 ports) OR router with N LAN ports (see the Networking section below) N short \u0026ldquo;patch\u0026rdquo; ethernet cables Power Supply (choose one)  N USB C power supplies N/4 4-port USB power supplies with N USB C cables N/4 BitScope Quattro Raspberry Pi blades and power supply   1 USB Drive [optional] 1 4-slot case (with heatsinks) [optional] 1 power strip [optional]  While you can use older models of the Pi if you already have them, using the most recent version will provide the most performance at the same price. Just make sure you get power cables that are compatible with your nodes.\nYou can also use larger RAM versions, but any amount of RAM should work for a minimally functional cluster. The more memory on your nodes, the larger problems they can solve and more performant they can be (caches for network and local storage and a reduction in swappiness).\nPut together the nodes If you got the BitScope Quattro for power or a case for your Pis, you will want to to get your Pis in place. This is also a great time to put on any heatsinks you have for your Pis.\nI would also recommend taking this time to decide the identity of each node and labeling them with a number or other identifier. I\u0026rsquo;ve decided to use numbers to identify my nodes, so I will use a marker or label to indicate which node is which number. This makes troubleshooting easier later on.\nConnect the wires Once your Pis are all ready to go, we need to connect them to power and network. It is useful to connect power and network cables in the order of the Pis so troubleshooting is easier when something goes wrong. Be sure to make sure all the cables are fully inserted.\nNetworking Connections For networking, you can take two paths:\n Use just a switch and connect the cluster to your home network Use a switch and/or a router to create a dedicated sub-network for your cluster. (You can use a switch to connect more nodes to your router if you have run out of ports on it)   I\u0026rsquo;ll be doing the second option as it give better separation from my other devices and allows me to set private IP addresses for my nodes regardless the IPs already in use on my home network.\nRegardless the path your choose, you will need to connect your switch or router\u0026rsquo;s WAN port to your home network so your cluster can access the internet and you can access your nodes. (You could also have your cluster completely air-gapped and use static IPs on the nodes, but not being able to download applications and tools is in my opinion not worth the effort).\nSoftware For this cluster I will be using Ubuntu. Canonical ( the company behind Ubuntu) has done a great job of ensuring Ubuntu is stable on Raspberry Pis (with the help of software from the Raspberry Pi Foundation) and has a 64 bit version available (unlike Raspberry Pi OS as of the time of writing). I will be using 20.04, but the latest LTS version should be fine.\nThere is already a great tutorial on how to install Ubuntu on a Raspberry Pi. Make sure to select the latest LTS version with 64 bit support. Also, we have no need to install a desktop, so you can skip that step.\nConnecting to the nodes If you followed the above tutorial, you should have the IP address of all your nodes. If you can\u0026rsquo;t tell which IP goes to which node, try unplugging the network cables from all but one node, follow the instructions, and repeat for all the other nodes. If you are using a router for your cluster, make sure you are connected to its network (its WiFi or LAN port) and not your home network as the router will block connections from your home network into your cluster network. (if you want, you can create a port forward on your cluster router for port 22 to your so you can SSH into)\nOnce you know what node is what IP address, connect to the first node (which we will use as our head node). Try running ping 1.1.1.1 to ensure your node can connect to the internet. Then follow the cluster SSH guide to setup SSH between all your nodes.\nStatic IP addresses No matter if you have a dedicated cluster network or it is connected to your home network, you should configure static IP addresses for all your nodes so their addresses will not change accidentally in the future.\nPackages In future posts we will install needed packages for configuring our cluster operation, but below are some useful packages that can help with troubleshooting and analyzing cluster performance.\nDon\u0026rsquo;t forget to sudo apt update to make sure you have the latest package database.\n htop iftop iotop dstat pv  ","description":"The basics of getting a cluster of Raspberry Pis powered on and running. Full cluster configuration in later posts.","id":6,"section":"posts","tags":["cluster","networks","hardware"],"title":"Basic Cluster Setup","uri":"https://johnhollowell.com/blog/posts/basic-cluster-setup/"},{"content":"Clemson\u0026rsquo;s School of Computing (SoC) is the place at Clemson where Computer Science (CPSC), Computer Information Systems (CIS), and Digital Production Arts (DPA) are located. Other computing departments (like Computer Engineering) also use some of the SoC\u0026rsquo;s systems. Below are some useful tips and tools for quickly getting going in the SoC.\nAccess Servers The access servers are the way you can access all the SoC computers from off-campus (without having to use the VPN). You can SSH into them and then SSH into other computers through access (or anything else you can do through SSH). You can connect to the access servers using ssh \u0026lt;clemson_username\u0026gt;@access.computing.clemson.edu (or just ssh access.computing.clemson.edu if you computer\u0026rsquo;s username matches your Clemson username). When you connect, you will see a list of lab computers that you can then connect to by using their name (e.g. ssh babbage1). You can also use access2.computing.clemson.edu if the main access server is down or overloaded.\nIf you are on campus, you can directly access the lab computers without the need to go through the access server. Simply use ssh \u0026lt;computer_name\u0026gt;.computing.clemson.edu while on campus (or VPN) and you can directly connect to the machine.\nNOTE: There is a limit in place on the number of connections for each user connecting to the access server. I\u0026rsquo;ve found it to be 4 connections. If you need more connections, consider using both access and access2 or using SSH Multiplexing. Files on the lab computers All the lab computers share your home directory. This means that if you write a file on one computer, you can access it on any other lab computer. This also means your settings for most programs will be the same on all the computers.\nThis also means you can access these files from your own computer as a network drive. Check out these instructions for more information on the subject (use the linux share instructions).\nSSH between computers SSHing between the lab machines can be a bit of a pain when you have to enter your password every time. It also makes it harder to write scripts that use multiple lab computers to work on rendering a project or running some processing. However, if you set up SSH keys on the computers, it allows the lab machines to connect to each other without the need for a password. And since the lab computers share files, once SSH keys are setup on one system, the will work on all the systems.\nThe process of making the keys we will use is fairly straight forward. You can check out more information on what these commands do if you are interested.\n1 2  ssh-keygen -t ed25519 -a 100 -f ~/.ssh/id_ed25519 -C \u0026#34;School of Computing\u0026#34; ssh-copy-id -i ~/.ssh/id_ed25519 localhost   This will generate a key for the computers to use, and \u0026ldquo;install\u0026rdquo; it so they will accept connections from that key. Since all the computers have the needed files due to the shared filesystem, all the computers now trust connections from all the other computers.\nSnapshot folder Oh no! You just deleted all the files for your assignment! Not to worry.\nYou home directory (/home/\u0026lt;username\u0026gt;/) on the SoC computers is backed up for just such a problem. Within every folder in your home directory is a hidden folder named .snapshot. It will not appear in any listing of directories, but if you cd into it, you can access all the different backups that are available. You can ls ~/.snapshot/ to see all the different dates that are have backups (there are hourly, daily, and weekly backups). These backup files are read-only, so you will need to copy them back into your home directory to be able to edit them.\nTo access and recover your files, you can either do\n1 2 3  cd ~ cd .snapshot/daily.1234-56-78_0010/path/to/your/files/ cp very-important-file.txt ~/path/to/your/files/   OR\n1 2 3  cd ~/path/to/your/files/ cd .snapshot/daily.1234-56-78_0010 cp very-important-file.txt ~/path/to/your/files/   Teachers' Office Hours While is isn\u0026rsquo;t really a technology in the SoC, your teachers are one of best resources to gain knowledge and software development skills. After all, the aren\u0026rsquo;t called teachers for nothing.\nAll teachers are required to have office hours (and so are Teaching Assistants (TAs)). Make use of this time to get to know your teacher, ask questions, and learn more about topics that excite you. It is also a good idea to start projects early (I\u0026rsquo;m not saying I ever did this, but it is what I should have done) so you can ask the teacher questions in office hours before everyone else starts to cram the assignment and office hours get busy.\nYOUR SUGGESTION HERE Is there something you really liked or have often used that you think I should add here or in another post? Get in contact with me and let me know!\n","description":"Clemson's School of Computing can be complicated. Here are some tips and tricks to get started quickly and make the most of the resources you have.","id":7,"section":"posts","tags":[""],"title":"Clemson SoC 101","uri":"https://johnhollowell.com/blog/posts/clemson-soc-101/"},{"content":"Clemson Universityâs computer labs store files across all the computers using network shares. You usually just access these shares on the lab machines, but you can also add the shares on your own computer as a network drive.\nThere are two main shares on campus: the campus share used by all the Windows (and Mac?) lab machines (e.g. in Cooper Library, Martin, etc.) and the School of Computingâs Linux systems. Both systems can be accessed in a similar way, but with different settings.\nTo access these network shares, you must either be on campus internet (WiFi or Ethernet) or have the Clemson VPN installed and activated on your device. See the CCIT guide for VPN access for more information.  The following instructions assume you are using a Windows device to access the shares. Using the credentials as below, you can follow a guide for adding network drives on Mac OS X or Linux (Ubuntu)\nSteps  Open File Explorer and go to \u0026ldquo;This PC\u0026rdquo;.  Click \u0026ldquo;Map Network Drive\u0026rdquo; in the top ribbon. Choose what drive letter you want the share to appear as (it doesnât matter what you choose for this; I used \u0026ldquo;Z\u0026rdquo; for this example)  Linux Share Windows Share  Enter \\\\neon.cs.clemson.edu\\home into the \u0026ldquo;folder\u0026rdquo; box.    5. Check both \"Reconnect as sign-in\" and \"Connect using different credentials\" so the network drive will automatically connect and you can use your Clemson credentials (rather than your local deviceâs username and password). Click \"Finish\". 6. Enter your University username (with @clemson.edu) and password. (You might have to click \"more choices\" in the login window to be able to enter a new username/password.)  7. Click \"OK\". Your School of Computing home directory should now appear under the drive letter you chose. NOTE: When adding new files via the network share, they are created with permissions defined by your umask. You can use chmod xxx \u0026lt;file\u0026gt; to change a files permissions to xxx (view a chomod guide for more information on the chmod command)  Enter \\\\home.clemson.edu\\\u0026lt;username\u0026gt; where \u0026lt;username\u0026gt; is your university username.    5. Check both \"Reconnect as sign-in\" and \"Connect using different credentials\" so the network drive will automatically connect and you can use your Clemson credentials (rather than your local deviceâs username and password). Click \"Finish\". 6. Enter your University username (without @clemson.edu) and password. (You might have to click \"more choices\" in the login window to be able to enter a new username/password.)  7. Click \"OK\". Your Windows home directory should now appear under the drive letter you chose.    'use strict'; var containerId = JSON.parse(\"\\\"35b5f9ea305eeab1\\\"\"); var containerElem = document.getElementById(containerId); var tabLinks = null; var tabContents = null; var ids = []; if (containerElem) { tabLinks = containerElem.querySelectorAll('.tab__link'); tabContents = containerElem.querySelectorAll('.tab__content'); } for (var i = 0; i 0) { tabContents[0].style.display = 'block'; }  You now have access to your files as if they were just another drive in your computer. Do note that these drives will be significantly slower than your actual computer drives due to higher latency and lower bandwidth.\n","description":"Clemson Universityâs computer labs store files across all the computers using network shares. You usually just access these shares on the lab machines, but you can also add the shares on your own computer as a network drive.","id":8,"section":"posts","tags":null,"title":"Accessing Your Clemson Network Shares","uri":"https://johnhollowell.com/blog/posts/accessing-your-clemson-network-shares/"},{"content":"Hey There! I\u0026rsquo;m John. I enjoy coding and problem solving. On the side I do some photography and videography work.\nCheck out my main website for more information about me and to get in contact.\n","description":"","id":9,"section":"","tags":null,"title":"About","uri":"https://johnhollowell.com/blog/about/"},{"content":"I\u0026rsquo;m at my extended family\u0026rsquo;s house way out in the middle of nowhere; barely enough cellular connection for a SMS, let alone trying to use any data.\nThey have DSL, but they are so far out that the signal is poor and it also is horrible speed. The fastest I saw while I was there was 700Kbps.\nWhile it is always a shock to go from over 100Mbps to under 1Mbps, I think that we are in an age where low bandwidth is not a show-stopper. Now obviously, downloading large files and games is a lot more tedious, I have found the \u0026ldquo;set everything to download overnight\u0026rdquo; method works quite well.\nI think there are three main reason why you can do more with less bandwidth than ever before.\nCompression and Codecs We have reached the point where processing power is so cheap, most of the time everything else is the limitation. We are glad to spend some power and time compressing data if it means we have more storage space on our devices or use less data. Website analysis tools will now complain if a webserver doesn\u0026rsquo;t compress its responses with at least gzip.\nWe are (slowly) starting to use new video and audio codecs that compress the crap out of the video/audio stream. Many devices are even starting to have highly performant hardware acceleration for these formats so it doesn\u0026rsquo;t even cause high load or power draw on mobile devices. Services like YouTube automatically convert content to many different qualities and have algorithms to pick the best quality that you can support.\nCaches, CDNS, and Apps Every web browser has a cache. Many even have several tiers of cache to give good hit/miss ratios and speed. If you are going to Facebook, you really should only ever need to receive the logo, most styles, and even some content once. This not only helps on slow connections, but even on fast connections an additional resource request can take a (relatively) long time to do an entire TCP and SSL handshake transaction.\nA further performance increase can be gained through websites' use of CDNs for their libraries and assets. If you are loading jQuery, FontAwesome, or bootstrap from local, you are doing it wrong. Pulling these assets from a CDN not only reduces the load on your server and the latency of the client accessing the resource, but allows caching these common resource between sites. If you visit a site using version x of the y library and then visit another site that uses the same version of y, you should be able to cache the first request of that resource and reuse it for any subsequent pages in any site. You can only do this if you using a CDN (and the same, but realistically most resources either have their own CDN or use one of the most common CDNs that everyone else uses).\nAdditionally, the use of site-specific apps (while annoying) allows the apps to only pull new content and \u0026ldquo;cache\u0026rdquo; all the resources needed to display the app. This makes it assured that outside of app updates, all most of the app\u0026rsquo;s traffic is the content you want to see (or ads sigh).\nMobile Focused Pages Thanks the the horrible practices of the Cellular Companies, anything that is loaded on a cellular connection needs to be small to not use much data to fit within limited bandwidth and even more limited data caps. While I have a great distaste for the stupidity of Cell carriers, their limitations have forced encouraged developments in efficient compression and transmission of pages (as well as a lot of bad practices in lazy loading and obfuscating in the name of minifying). Mosts sites will load smaller or more compressed assets when they detect they are on mobile platforms.\nCaveats While I did \u0026ldquo;survive\u0026rdquo; on the limited connection, I knew it was coming and was able to prepare a bit for it. I downloaded a couple of additional playlists on Spotify and synced a few episodes of TV to my phone from my Plex. However, I did not even use these additional downloads. I used the podcasts I had previously downloaded and even downloaded an additional episode while there. The ability in most apps to download content makes even a trickle of internet be enough to slowly build up the content you want.\nI have also recently reset my laptop and had to download FFmpeg while there. It took a few minutes, but it didn\u0026rsquo;t fail. I did want to do some complex computing while there, but since most of what I do is on other computers (servers, remote machines, etc) it was incredibly easy to do what I wanted to do through an SSH connection to a datacenter. This is cheating a little bit but really is not out of the ordinary; even on fast internet I would SSH out to do things I didn\u0026rsquo;t want or couldn\u0026rsquo;t do on my device (thanks Windows). This not not that different from devices like Chromebooks which almost entirely run remotely and require an internet connection to function (or function with all features).\nThis was also a family gathering, so I didn\u0026rsquo;t spend much time on the internet. I could quickly google the answer to win an argument and that was all I needed.\nConclusion Slow internet is still a pain, but I\u0026rsquo;ve grown to appreciate its limitations and work around them. Several trends in computing and content delivery in recent years have made slow internet more bearable. I won\u0026rsquo;t be giving up my high-speed internet any time soon, but slowing down and disconnecting a bit is a nice change of pace in this time where everything has to happen online.\n","description":"While it is always a shock to go from over 100Mbps to under 1Mbps, I think that we are in an age where low bandwidth is not a show-stopper.","id":10,"section":"posts","tags":["web","life","opinion"],"title":"A Trickle Is Better Than Nothing","uri":"https://johnhollowell.com/blog/posts/a-trickle-is-better-than-nothing/"},{"content":"2021. A new year; a new start.\nI\u0026rsquo;ve wanted to start a blog for a while, and I thought I might as well start it on the first of the year. I think I finally have enough things I want to talk about that a blog is worth the effort.\nWhat\u0026rsquo;s in a Name? So why the name \u0026ldquo;/dev/random\u0026rdquo;? Well, I\u0026rsquo;m a geek and this blog will be about anything. I don\u0026rsquo;t want to confine this blog to any one subject (including to just tech) and I want the entirety of the blog to be representative of that. It also give me the opportunity to have a punny subtitle, which I am always appreciative of.\nSo\u0026hellip; Why? This blog is mostly a place for me to put information for my future self and others. Don\u0026rsquo;t expect any deep, rambling prose. I\u0026rsquo;m not a spectacular writer and there are many things in my life that don\u0026rsquo;t merit blogging about. However, I have a vary wide range of knowledge which I often will forget by the next time I need to use it. This gives me a way to record my experiences and experiments in a public place to which I can reference others. This blog is also an experiment, how meta is that?\nWhen can I get more of this great content? I would like to at least work on this blog every day. That doesn\u0026rsquo;t mean a new post every month; longer and more detailed posts will take me a bit longer. I might hold a post so a whole series can be release together. I might get bored and never create another post. Who knows?\n","description":"I've wanted to start a blog for a while, and I thought I might as well start it on the first of the year. I think I finally have enough things I want to talk about that a blog is worth the effort.","id":11,"section":"posts","tags":null,"title":"And So It Begins","uri":"https://johnhollowell.com/blog/posts/and-so-it-begins/"}]